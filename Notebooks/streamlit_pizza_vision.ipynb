{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pizza_vision.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CUfgvQspdU6",
        "outputId": "b1518815-c970-4b4f-8816-579bffcbeabe"
      },
      "source": [
        "import streamlit as st\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from numpy.linalg import norm\r\n",
        "import time\r\n",
        "import random\r\n",
        "from datetime import datetime\r\n",
        "import re\r\n",
        "import os\r\n",
        "import urllib, io\r\n",
        "from io import StringIO \r\n",
        "import string\r\n",
        "\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "# from nltk import sent_tokenize, word_tokenize\r\n",
        "# from nltk.stem.snowball import SnowballStemmer\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import string\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.corpus import stopwords, wordnet\r\n",
        "import pickle\r\n",
        "# from tqdm import tqdm, tqdm_notebook\r\n",
        "import sklearn\r\n",
        "# from sklearn.manifold import TSNE\r\n",
        "# from sklearn.decomposition import PCA\r\n",
        "from sklearn.metrics import pairwise_distances\r\n",
        "\r\n",
        "import PIL\r\n",
        "from PIL import Image\r\n",
        "from sklearn.neighbors import NearestNeighbors\r\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\r\n",
        "from tensorflow.keras.preprocessing import image as image_process\r\n",
        "\r\n",
        "import glob\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.image as mpimg\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# Set stop words for cleaning input text\r\n",
        "stops = stopwords.words(\"english\")\r\n",
        "sw = stops + ['pizza','pizzas','\\xa0we','come', 'food', 'one', 'give', 'ask', 'back', 'great', 'take', 'wait','bar','pasta','time','place','go','would','say','call','make','minute','like','miss','pita','rib','salad','gyro','tzatziki','wing','burger','chicken','sandwich','dog','steak','hotdog']\r\n",
        "\r\n",
        "# Pizza info\r\n",
        "big_pizza_details = pickle.load(open('/app/pizza_vision/web_app/big_pizza_details.pickle', 'rb'))\r\n",
        "url_df = pickle.load(open('/app/pizza_vision/web_app/big_pizza_urls.pickle', 'rb'))\r\n",
        "chicagoland = pickle.load(open('/app/pizza_vision/web_app/chicagoland.pickle', 'rb'))\r\n",
        "\r\n",
        "# Computer Vision\r\n",
        "filenames = pickle.load(open('/app/pizza_vision/web_app/OGfilenames_images.pickle', 'rb'))\r\n",
        "feature_list = pickle.load(open('/app/pizza_vision/web_app/OGfeatures-resnet.pickle','rb'))\r\n",
        "class_ids = pickle.load(open('/app/pizza_vision/web_app/OGresnet_classids.pickle', 'rb'))\r\n",
        "# apart_features = pickle.load(open('/content/drive/My Drive/ds/pizza_images/autoencoder/features-resnet-apart.pickle', 'rb'))\r\n",
        "\r\n",
        "# NLP info\r\n",
        "nmf_df = pickle.load(open('/app/pizza_vision/web_app/colab_nmf_df.pickle', 'rb'))\r\n",
        "nmf = pickle.load(open('/app/pizza_vision/web_app/colab_nmf.pickle', 'rb'))\r\n",
        "doc_topic = pickle.load(open('/app/pizza_vision/web_app/colab_doc_topic.pickle', 'rb'))\r\n",
        "topic_word = pickle.load(open('/app/pizza_vision/web_app/colab_topic_word.pickle', 'rb'))\r\n",
        "tfidf = pickle.load(open('/app/pizza_vision/web_app/colab_tfidf.pickle', 'rb'))\r\n",
        "tfidf__mat = pickle.load(open('/app/pizza_vision/web_app/colab_tfidf_mat.pickle', 'rb'))\r\n",
        "\r\n",
        "resnet_model = ResNet50(weights='imagenet',include_top=False, input_shape=(224, 224, 3),pooling='max')\r\n",
        "\r\n",
        "# Helper function to get the classname\r\n",
        "def classname(str):\r\n",
        "    return str.split('/')[-2]\r\n",
        "\r\n",
        "\r\n",
        "# Helper function to get the classname and filename\r\n",
        "def classname_filename(str):\r\n",
        "    return str.split('/')[-2] + '/' + str.split('/')[-1]\r\n",
        "\r\n",
        "\r\n",
        "# Helper functions to plot the nearest images given a query image\r\n",
        "def plot_images(filenames, distances):\r\n",
        "    images = []\r\n",
        "    rec_ids = []\r\n",
        "    for filename in filenames:\r\n",
        "        images.append(mpimg.imread(filename))\r\n",
        "    plt.figure(figsize=(20, 10))\r\n",
        "    columns = 4\r\n",
        "    for i, image in enumerate(images):\r\n",
        "        ax = plt.subplot(len(images) / columns + 1, columns, i + 1)\r\n",
        "        if i == 0:\r\n",
        "            ax.set_title(\"Query Image\\n\" + filenames[i].split('/')[-1].split('.')[0])\r\n",
        "        else:\r\n",
        "            ax.set_title(\"Similar Image\\n\" + filenames[i].split('/')[-1].split('.')[0] +\r\n",
        "                         \"\\nDistance: \" +\r\n",
        "                         str(float(\"{0:.2f}\".format(distances[i]))))\r\n",
        "        plt.imshow(image)\r\n",
        "\r\n",
        "# Helper function to return restaurant ids for recommendations\r\n",
        "def get_image_recs(img_path, num_recs):\r\n",
        "  img_features = extract_features(img_path,resnet_model)\r\n",
        "  distances, indices = neighbors.kneighbors([img_features])\r\n",
        "  # Since this image is from outside our images, first image is ok to take as recommendation\r\n",
        "  similar_image_paths = [filenames[indices[0][i]] for i in range(0, num_recs)]\r\n",
        "  rec_ids = []\r\n",
        "  for filename in similar_image_paths:\r\n",
        "    rec_ids.append(filename.split('/')[-1].split('.')[0].split('_')[0])\r\n",
        "  return rec_ids\r\n",
        "\r\n",
        "# Helper function to extract resnet features from an image\r\n",
        "def extract_features(img, model):\r\n",
        "    input_shape = (224, 224, 3)\r\n",
        "    img_array = image_process.img_to_array(img)\r\n",
        "    expanded_img_array = np.expand_dims(img_array, axis=0)\r\n",
        "    preprocessed_img = preprocess_input(expanded_img_array)\r\n",
        "    features = model.predict(preprocessed_img)\r\n",
        "    flattened_features = features.flatten()\r\n",
        "    normalized_features = flattened_features / norm(flattened_features)\r\n",
        "    return normalized_features\r\n",
        "\r\n",
        "# Helper function to clean user input text before TFIDF\r\n",
        "def clean_text(input):\r\n",
        "  total_df = pd.DataFrame([input],columns=['pizza_words'])\r\n",
        "  total_df['tokenized'] = total_df['pizza_words'].apply(word_tokenize)\r\n",
        "  total_df['lower'] = total_df['tokenized'].apply(lambda x: [word.lower() for word in x])\r\n",
        "  punc = string.punctuation\r\n",
        "  total_df['no_punc'] = total_df['lower'].apply(lambda x: [word for word in x if word not in punc])\r\n",
        "  total_df['stopwords_removed'] = total_df['no_punc'].apply(lambda x: [word for word in x if word not in sw])\r\n",
        "  total_df['pos_tags'] = total_df['stopwords_removed'].apply(nltk.tag.pos_tag)\r\n",
        "  def get_wordnet_pos(tag):\r\n",
        "      if tag.startswith('J'):\r\n",
        "          return wordnet.ADJ\r\n",
        "      elif tag.startswith('V'):\r\n",
        "          return wordnet.VERB\r\n",
        "      elif tag.startswith('N'):\r\n",
        "          return wordnet.NOUN\r\n",
        "      elif tag.startswith('R'):\r\n",
        "          return wordnet.ADV\r\n",
        "      else:\r\n",
        "          return wordnet.NOUN\r\n",
        "  total_df['wordnet_pos'] = total_df['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\r\n",
        "  wnl = WordNetLemmatizer()\r\n",
        "  total_df['lemmatized'] = total_df['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\r\n",
        "  total_df['clean_pizza'] = [' '.join(map(str, l)) for l in total_df['lemmatized']]\r\n",
        "  total_df['clean_pizza']  = total_df['clean_pizza'] .str.replace(r'\\d+','',regex=True) # remove numbers\r\n",
        "  clean_string = total_df.iloc[0,0]\r\n",
        "  return clean_string\r\n",
        "\r\n",
        "st.sidebar.write(\r\n",
        "    '''\r\n",
        "    __About__ \\n\r\n",
        "    This project was built from just under 1000 scraped restaurants in the Chicagoland area. The user reviews were used to create vectors across the pizza spectrum for comparisons between pizzas. \r\n",
        "    \\n\r\n",
        "    This site was created by Ethan Feldman. You can find him on [GitHub](https://github.com/ejfeldman7), [LinkedIn](https://www.linkedin.com/in/feldmanethan/), [Medium/TDS](https://ethan-feldman.medium.com/) and his [website](https://www.ejfeldman.com/).\r\n",
        "    ''')\r\n",
        "st.title('Pizza-Vision')\r\n",
        "st.write('A few years ago, our favorite pizzeria closed and ever since, my wife and I have not been able to find a new pizza that matched the same inocuous style. In an attempt to find new pizza, I created the recommendation system that filters by similar images and then recommends based on similarity to user reviews.')\r\n",
        "st.write('To use this recommender, try adding an image and a description of the pizza you want. Try to think about the style, the crust type, flavors, and more in your description.')\r\n",
        "st.write('You must add text and an image to get a recommendation')\r\n",
        "user_text = st.text_input(\"Write a couple sentences (the more the better) to describe your pizza\", '')\r\n",
        "\r\n",
        "st.title(\"Upload + Classification Example\")\r\n",
        "\r\n",
        "uploaded_file = st.file_uploader(\"Choose an image...\", type=\"jpg\")\r\n",
        "\r\n",
        "if ((uploaded_file is not None) & (user_text != '')):\r\n",
        "    # user_image = Image.open(uploaded_file)\r\n",
        "    # image_other = image.load_img(uploaded_file,target_size=(input_shape[0], input_shape[1]))\r\n",
        "    image = Image.open(uploaded_file)\r\n",
        "    newsize = (224, 224) \r\n",
        "    image = image.resize(newsize) \r\n",
        "    st.image(image, caption='Uploaded Image.', use_column_width=True)\r\n",
        "    st.write(\"\")\r\n",
        "    st.write(\"Working on a recommendation...\")\r\n",
        "\r\n",
        "    # Get the 25 closest images to the input using Nearest Neighbors by Euclidean distance\r\n",
        "    neighbors = NearestNeighbors(n_neighbors=25, algorithm='brute', metric='euclidean').fit(feature_list)\r\n",
        "\r\n",
        "    # Get 25 recommended images for an image (placeholder file entered for now)\r\n",
        "    image_recs = get_image_recs(image,25)\r\n",
        "\r\n",
        "    # Use this to find and show the top three images, along with the uploaded image (placeholder files for now)\r\n",
        "    distances, indices = neighbors.kneighbors([extract_features(image,resnet_model)])\r\n",
        "\r\n",
        "    # Since this image is from outside our images, first image is ok to take as recommendation\r\n",
        "    similar_image_paths = [uploaded_file] + [filenames[indices[0][i]] for i in range(0, 3)]\r\n",
        "    # plot_images(similar_image_paths, distances[0])\r\n",
        "\r\n",
        "    # Get dataframe of 25 recommended pizzas from full restaurant list\r\n",
        "    image_recs_df = nmf_df[nmf_df['id'].isin(image_recs) & (nmf_df['pizza_words'] != '')].reset_index()\r\n",
        "\r\n",
        "    # Get user text input (placeholder for now) and clean it for topic modeling\r\n",
        "    user_text = clean_text(user_text)\r\n",
        "    # Vectorize user text, do topic modeling\r\n",
        "    vt = tfidf.transform([user_text]).todense() # \r\n",
        "    tt1 = nmf.transform(vt)\r\n",
        "    doc_topic = image_recs_df[['Delivery','Italian','Deep Dish','Pizza Puffs','NY/Detroit','Tavern Style','Bar Food']]\r\n",
        "\r\n",
        "    # Find cosine distances between image recommendations and input text\r\n",
        "    indices = pairwise_distances(tt1.reshape(1,-1),doc_topic,metric='cosine').argsort()\r\n",
        "\r\n",
        "    # Select the top three closest user reviews with the input text and find those restaurants\r\n",
        "    recs = list(indices[0][0:4])\r\n",
        "    # image_recs_df.iloc[recs]\r\n",
        "\r\n",
        "    # Report back the final recommendations\r\n",
        "    st.write('Based on your image and text description, the following options are recommended:') #str(item)\r\n",
        "    st.write('\\n')\r\n",
        "    st.write('I recommend you try:',image_recs_df.iloc[recs[0]]['name'],'located at',image_recs_df.iloc[recs[0]]['address'],'.')\r\n",
        "    st.write('\\n')\r\n",
        "    st.write('I recommend you try:',image_recs_df.iloc[recs[1]]['name'],'located at',image_recs_df.iloc[recs[1]]['address'],'.')\r\n",
        "    st.write('\\n')\r\n",
        "    st.write('I recommend you try:',image_recs_df.iloc[recs[2]]['name'],'located at',image_recs_df.iloc[recs[2]]['address'],'.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G4EUv48poIQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}