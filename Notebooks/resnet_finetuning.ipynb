{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet_finetuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2b07e7e50e9641da9b1bd8b0271d94b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7b47be403db4442189c381190bbd68a2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3d4ac48c75f94efe966c17105bf227b0",
              "IPY_MODEL_6f169e518eda46218cc1c4f5226f99f6"
            ]
          }
        },
        "7b47be403db4442189c381190bbd68a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3d4ac48c75f94efe966c17105bf227b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f726adbea5e14983939eca822ba78849",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2023,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2023,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0629386cffde453f9da417250e5f2216"
          }
        },
        "6f169e518eda46218cc1c4f5226f99f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ac5e5bb601aa4fe6b013871653834414",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2023/2023 [8:09:18&lt;00:00, 14.51s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a4eb579db1ab4b74b9ee899bde58d8f4"
          }
        },
        "f726adbea5e14983939eca822ba78849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0629386cffde453f9da417250e5f2216": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ac5e5bb601aa4fe6b013871653834414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a4eb579db1ab4b74b9ee899bde58d8f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc9152a379504a63a1c1e56b7204f79c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e85a36896d1d4d53b7ff79ea1959b855",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1541b704570e48c48beb8502d9c4d6e4",
              "IPY_MODEL_fcfaeaed3fd642608b8aa5dff2253913"
            ]
          }
        },
        "e85a36896d1d4d53b7ff79ea1959b855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1541b704570e48c48beb8502d9c4d6e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_591483d42c5945a6b59fede3e5528284",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1698,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1698,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_533d5777afc24e6996f467a9025501be"
          }
        },
        "fcfaeaed3fd642608b8aa5dff2253913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eb3f38e391ea4da2a6f482041ab11f44",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1698/1698 [08:09&lt;00:00,  3.47it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2c0c786da6ad49958a513735e228e1f8"
          }
        },
        "591483d42c5945a6b59fede3e5528284": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "533d5777afc24e6996f467a9025501be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb3f38e391ea4da2a6f482041ab11f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2c0c786da6ad49958a513735e228e1f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pr34WmpqKfJ",
        "outputId": "ed7a9339-ec40-47a9-ebcf-ded9769baaa8"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCTZ1NDIqY8K"
      },
      "source": [
        "import numpy as np\r\n",
        "from numpy.linalg import norm\r\n",
        "import pickle\r\n",
        "from tqdm import tqdm, tqdm_notebook\r\n",
        "import os\r\n",
        "import random\r\n",
        "import time\r\n",
        "import math\r\n",
        "import tensorflow\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\r\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\r\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\r\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet\r\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyLyghrWFxVc"
      },
      "source": [
        "import cv2\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torchvision.models as models\r\n",
        "import torchvision.transforms as transforms\r\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTUh6bq8qfl8"
      },
      "source": [
        "\r\n",
        "def model_picker(name):\r\n",
        "    if (name == 'vgg16'):\r\n",
        "        model = VGG16(weights='imagenet',\r\n",
        "                      include_top=False,\r\n",
        "                      input_shape=(224, 224, 3),\r\n",
        "                      pooling='max')\r\n",
        "    elif (name == 'vgg19'):\r\n",
        "        model = VGG19(weights='imagenet',\r\n",
        "                      include_top=False,\r\n",
        "                      input_shape=(224, 224, 3),\r\n",
        "                      pooling='max')\r\n",
        "    elif (name == 'mobilenet'):\r\n",
        "        model = MobileNet(weights='imagenet',\r\n",
        "                          include_top=False,\r\n",
        "                          input_shape=(224, 224, 3),\r\n",
        "                          pooling='max',\r\n",
        "                          depth_multiplier=1,\r\n",
        "                          alpha=1)\r\n",
        "    elif (name == 'inception'):\r\n",
        "        model = InceptionV3(weights='imagenet',\r\n",
        "                            include_top=False,\r\n",
        "                            input_shape=(224, 224, 3),\r\n",
        "                            pooling='max')\r\n",
        "    elif (name == 'resnet'):\r\n",
        "        model = ResNet50(weights='imagenet',\r\n",
        "                         include_top=False,\r\n",
        "                         input_shape=(224, 224, 3),\r\n",
        "                        pooling='max')\r\n",
        "    elif (name == 'xception'):\r\n",
        "        model = Xception(weights='imagenet',\r\n",
        "                         include_top=False,\r\n",
        "                         input_shape=(224, 224, 3),\r\n",
        "                         pooling='max')\r\n",
        "    else:\r\n",
        "        print(\"Specified model not available\")\r\n",
        "    return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLD4x-Qwqiih",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "675f0cce-c226-48fa-9d82-a60eb156af99"
      },
      "source": [
        "model_architecture = 'resnet'\r\n",
        "model = model_picker(model_architecture)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSjcHO8YqlMQ"
      },
      "source": [
        "def extract_features(img_path, model):\r\n",
        "    input_shape = (224, 224, 3)\r\n",
        "    img = image.load_img(img_path,\r\n",
        "                         target_size=(input_shape[0], input_shape[1]))\r\n",
        "    img_array = image.img_to_array(img)\r\n",
        "    expanded_img_array = np.expand_dims(img_array, axis=0)\r\n",
        "    preprocessed_img = preprocess_input(expanded_img_array)\r\n",
        "    features = model.predict(preprocessed_img)\r\n",
        "    flattened_features = features.flatten()\r\n",
        "    normalized_features = flattened_features / norm(flattened_features)\r\n",
        "    return normalized_features"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZzRvygRqsUg",
        "outputId": "b1dd00b9-2591-4d05-c075-e8935ecb5bc2"
      },
      "source": [
        "features = extract_features('/content/drive/MyDrive/ds/pizza_images/all_images/data/amatos-pizza-and-more-chicago_0.jpeg', model)\r\n",
        "apart = extract_features('/content/drive/MyDrive/ds/pizza_images/autoencoder/test/veranda_test.jpg', model)\r\n",
        "print(len(features))\r\n",
        "print(apart)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2048\n",
            "[0.00178475 0.00101728 0.00843327 ... 0.00108938 0.00463534 0.03208936]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3x-Ra4ZFS3S",
        "outputId": "93f896b3-5b4e-4967-db37-886ad275137a"
      },
      "source": [
        "apart_fine = extract_features('/content/drive/MyDrive/ds/pizza_images/autoencoder/test/veranda_test.jpg', model_finetuned)\r\n",
        "pickle.dump(apart,open('/content/drive/My Drive/ds/pizza_images/autoencoder/features-resnet-apart.pickle', 'wb'))\r\n",
        "print(len(apart_fine))\r\n",
        "print(apart_fine)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "[1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Liw-p8PN8Kk3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JC5op2c8xzDg"
      },
      "source": [
        "extensions = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG']\r\n",
        "\r\n",
        "def get_file_list(root_dir):\r\n",
        "    file_list = []\r\n",
        "    for root, directories, filenames in os.walk(root_dir):\r\n",
        "        for filename in filenames:\r\n",
        "            if any(ext in filename for ext in extensions):\r\n",
        "                file_list.append(os.path.join(root, filename))\r\n",
        "    return file_list"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "2b07e7e50e9641da9b1bd8b0271d94b5",
            "7b47be403db4442189c381190bbd68a2",
            "3d4ac48c75f94efe966c17105bf227b0",
            "6f169e518eda46218cc1c4f5226f99f6",
            "f726adbea5e14983939eca822ba78849",
            "0629386cffde453f9da417250e5f2216",
            "ac5e5bb601aa4fe6b013871653834414",
            "a4eb579db1ab4b74b9ee899bde58d8f4"
          ]
        },
        "id": "rtFvHa6Tqvfe",
        "outputId": "13c401ad-80fb-497d-8fd1-72978ab7827b"
      },
      "source": [
        "# path to the your datasets\r\n",
        "root_dir = '/content/drive/MyDrive/ds/pizza_images/all_images/data/'\r\n",
        "filenames = sorted(get_file_list(root_dir))\r\n",
        "\r\n",
        "feature_list = []\r\n",
        "for i in tqdm_notebook(range(len(filenames))):\r\n",
        "    feature_list.append(extract_features(filenames[i], model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b07e7e50e9641da9b1bd8b0271d94b5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2023.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVhrkhGTxsxm",
        "outputId": "25a48072-8505-4042-c216-dac9605865e2"
      },
      "source": [
        "batch_size = 64\r\n",
        "datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\r\n",
        "\r\n",
        "generator = datagen.flow_from_directory('/content/drive/MyDrive/ds/pizza_images/all_images/',\r\n",
        "                                        target_size=(224, 224),\r\n",
        "                                        batch_size=batch_size,\r\n",
        "                                        class_mode=None,\r\n",
        "                                        shuffle=False)\r\n",
        "\r\n",
        "num_images = len(generator.filenames)\r\n",
        "num_epochs = int(math.ceil(num_images / batch_size))\r\n",
        "\r\n",
        "start_time = time.time()\r\n",
        "feature_list = []\r\n",
        "feature_list = model.predict_generator(generator, num_epochs)\r\n",
        "end_time = time.time()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2023 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My3qXuFx-YOB",
        "outputId": "a18e25cd-91d2-41b2-8ff6-188668531aa0"
      },
      "source": [
        "for i, features in enumerate(feature_list):\r\n",
        "    feature_list[i] = features / norm(features)\r\n",
        "\r\n",
        "feature_list = feature_list.reshape(num_images, -1)\r\n",
        "filenames = [root_dir + '/' + s for s in generator.filenames]\r\n",
        "print(\"Num images   = \", len(generator.classes))\r\n",
        "print(\"Shape of feature_list = \", feature_list.shape)\r\n",
        "print(\"Time taken in sec = \", end_time - start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num images   =  2023\n",
            "Shape of feature_list =  (2023, 2048)\n",
            "Time taken in sec =  353.40863704681396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhLdL1JlPh0i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guIn5dMSPrLA"
      },
      "source": [
        "pickle.dump(generator.classes, open('/content/drive/My Drive/ds/pizza_images/autoencoder/OGresnet_classids.pickle',\r\n",
        "                                    'wb'))\r\n",
        "pickle.dump(filenames, open('/content/drive/My Drive/ds/pizza_images/autoencoder/OGfilenames_images.pickle', 'wb'))\r\n",
        "pickle.dump(\r\n",
        "    feature_list,\r\n",
        "    open('/content/drive/My Drive/ds/pizza_images/autoencoder/OGfeatures-' + model_architecture + '.pickle', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI-2mRAsyPS8",
        "outputId": "fb839c90-463b-4aa3-ae83-a3b7ee5c1c88"
      },
      "source": [
        "feature_list[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00599818, 0.01011654, 0.01637081, ..., 0.00585441, 0.01143654,\n",
              "       0.02530114], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MkSr3ePPrO8"
      },
      "source": [
        "batch_size = 64\r\n",
        "TRAIN_SAMPLES = 200\r\n",
        "NUM_CLASSES = 2\r\n",
        "IMG_WIDTH, IMG_HEIGHT = 224, 224"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXHFJvNirjRB"
      },
      "source": [
        "from shutil import copy2\r\n",
        "\r\n",
        "# Sample files used to train model drawn from all images\r\n",
        "sample_size = 200\r\n",
        "thin_pop = '/content/drive/MyDrive/ds/pizza_images/pizza_topics/thin/'\r\n",
        "thick_pop = '/content/drive/MyDrive/ds/pizza_images/pizza_topics/thick/'\r\n",
        "thin_sample = '/content/drive/MyDrive/ds/pizza_images/resnet_training/thin/'\r\n",
        "thick_sample = '/content/drive/MyDrive/ds/pizza_images/resnet_training/thick/'\r\n",
        "\r\n",
        "total_thin_count = len([name for name in os.listdir(thin_pop)])\r\n",
        "total_thick_count = len([name for name in os.listdir(thick_pop)])\r\n",
        "thin_image_names = os.listdir(thin_pop)\r\n",
        "thick_image_names = os.listdir(thick_pop)\r\n",
        "\r\n",
        "# thin_sample_indices = np.random.randint(0, total_thin_count, size=sample_size)\r\n",
        "# thick_sample_indices = np.random.randint(0, total_thick_count, size=sample_size)\r\n",
        "# thin_sample_indices = thin_sample_indices[:int(sample_size)]\r\n",
        "# thick_sample_indices = thick_sample_indices[:int(sample_size)]\r\n",
        "\r\n",
        "# for index in thin_sample_indices:\r\n",
        "#     copy2(thin_pop +  thin_image_names[index], thin_sample + thin_image_names[index])\r\n",
        "# for index in thick_sample_indices:\r\n",
        "#     copy2(thick_pop + thick_image_names[index], thick_sample + thick_image_names[index])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIV6PL71PyEw"
      },
      "source": [
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\r\n",
        "                                   rotation_range=70,\r\n",
        "                                   shear_range=0.2,\r\n",
        "                                   zoom_range=0.2,\r\n",
        "                                   horizontal_flip = True,\r\n",
        "                                   vertical_flip = True)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ci2M-07QgC4",
        "outputId": "c0420c32-5134-41ed-8dff-d09409dabb1e"
      },
      "source": [
        "batch_size=32\r\n",
        "train_generator = train_datagen.flow_from_directory('/content/drive/MyDrive/ds/pizza_images/resnet_training/',\r\n",
        "                                                    target_size=(IMG_WIDTH,\r\n",
        "                                                                 IMG_HEIGHT),\r\n",
        "                                                    batch_size=batch_size,\r\n",
        "                                                    shuffle=True,\r\n",
        "                                                    seed=12345,\r\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 332 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl4MJDJ-P60x"
      },
      "source": [
        "def model_maker():\r\n",
        "    base_model = ResNet50(include_top=False,\r\n",
        "                           input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\r\n",
        "    for layer in base_model.layers[:]:\r\n",
        "        layer.trainable = False\r\n",
        "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\r\n",
        "    custom_model = base_model(input)\r\n",
        "    custom_model = GlobalAveragePooling2D()(custom_model)\r\n",
        "    custom_model = Dense(64, activation='relu', name='encoded_layer')(custom_model)\r\n",
        "    custom_model = Dropout(0.5)(custom_model)\r\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\r\n",
        "    return Model(inputs=input, outputs=predictions)\r\n",
        "    # Can I just change outputs to be another layer?"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALtIeuQHP_3S",
        "outputId": "d646c7d1-136f-4d15-c87d-8434eada0814"
      },
      "source": [
        "model_finetuned2 = model_maker()\r\n",
        "model_finetuned2.compile(loss='mse',\r\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(0.001),\r\n",
        "              metrics=['acc'])\r\n",
        "history = model_finetuned2.fit_generator(\r\n",
        "    train_generator,\r\n",
        "    steps_per_epoch=math.ceil(float(TRAIN_SAMPLES) / batch_size),\r\n",
        "    epochs=20)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "7/7 [==============================] - 43s 6s/step - loss: 0.3616 - acc: 0.5042\n",
            "Epoch 2/20\n",
            "7/7 [==============================] - 41s 6s/step - loss: 0.2821 - acc: 0.6067\n",
            "Epoch 3/20\n",
            "7/7 [==============================] - 36s 5s/step - loss: 0.1849 - acc: 0.7584\n",
            "Epoch 4/20\n",
            "7/7 [==============================] - 36s 5s/step - loss: 0.2146 - acc: 0.7247\n",
            "Epoch 5/20\n",
            "7/7 [==============================] - 41s 6s/step - loss: 0.2275 - acc: 0.7109\n",
            "Epoch 6/20\n",
            "7/7 [==============================] - 41s 6s/step - loss: 0.1757 - acc: 0.7914\n",
            "Epoch 7/20\n",
            "7/7 [==============================] - 37s 6s/step - loss: 0.1451 - acc: 0.7967\n",
            "Epoch 8/20\n",
            "7/7 [==============================] - 38s 5s/step - loss: 0.1772 - acc: 0.7609\n",
            "Epoch 9/20\n",
            "7/7 [==============================] - 39s 5s/step - loss: 0.1521 - acc: 0.8021\n",
            "Epoch 10/20\n",
            "7/7 [==============================] - 42s 6s/step - loss: 0.1484 - acc: 0.8001\n",
            "Epoch 11/20\n",
            "7/7 [==============================] - 38s 5s/step - loss: 0.1392 - acc: 0.7945\n",
            "Epoch 12/20\n",
            "7/7 [==============================] - 41s 6s/step - loss: 0.1166 - acc: 0.8508\n",
            "Epoch 13/20\n",
            "7/7 [==============================] - 38s 5s/step - loss: 0.1524 - acc: 0.7991\n",
            "Epoch 14/20\n",
            "7/7 [==============================] - 38s 6s/step - loss: 0.1174 - acc: 0.8467\n",
            "Epoch 15/20\n",
            "7/7 [==============================] - 40s 6s/step - loss: 0.1208 - acc: 0.8510\n",
            "Epoch 16/20\n",
            "7/7 [==============================] - 38s 5s/step - loss: 0.1439 - acc: 0.8197\n",
            "Epoch 17/20\n",
            "7/7 [==============================] - 37s 5s/step - loss: 0.0988 - acc: 0.8736\n",
            "Epoch 18/20\n",
            "7/7 [==============================] - 37s 5s/step - loss: 0.1216 - acc: 0.8574\n",
            "Epoch 19/20\n",
            "7/7 [==============================] - 37s 5s/step - loss: 0.1454 - acc: 0.8094\n",
            "Epoch 20/20\n",
            "7/7 [==============================] - 38s 5s/step - loss: 0.1253 - acc: 0.8260\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMWPNedl_cA8",
        "outputId": "99b26ca2-a14a-4b8b-da2e-8ec3a028c578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "# Plot accuracy and loss for training and validation sets\r\n",
        "acc = history.history['acc']\r\n",
        "loss = history.history['loss']\r\n",
        "\r\n",
        "epochs = range(len(acc))\r\n",
        "\r\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\r\n",
        "plt.title('Training accuracy')\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "\r\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\r\n",
        "plt.title('Training loss')\r\n",
        "plt.legend()\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCF2C5yRQpyK"
      },
      "source": [
        "# model_finetuned.save('/content/drive/My Drive/ds/pizza_images/autoencoder/finetuned_resnet.h5')\r\n",
        "# model_finetuned.save('/content/drive/My Drive/ds/pizza_images/autoencoder/finetuned_resnet_thickthin_10.h5')\r\n",
        "# model_finetuned.save('/content/drive/My Drive/ds/pizza_images/autoencoder/finetuned_resnet_thickthin_bs32.h5')\r\n",
        "model_finetuned2.save('/content/drive/My Drive/ds/pizza_images/autoencoder/finetuned_resnet_thickthin_shear.h5')"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBybGhdQvw9h"
      },
      "source": [
        "model_finetuned1 =  tensorflow.keras.models.load_model('/content/drive/My Drive/ds/pizza_images/autoencoder/finetuned_resnet1.h5')\r\n",
        "model_finetuned2 =  tensorflow.keras.models.load_model('/content/drive/My Drive/ds/pizza_images/autoencoder/finetuned_resnet_thickthin_10.h5')\r\n",
        "model_finetuned3 =  tensorflow.keras.models.load_model('/content/drive/My Drive/ds/pizza_images/autoencoder/finetuned_resnet_thickthin_bs32.h5')\r\n",
        "model_finetuned4 =  tensorflow.keras.models.load_model('/content/drive/My Drive/ds/pizza_images/autoencoder/finetuned_resnet_thickthin_shear.h5')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xv3g7BCRSF3I",
        "outputId": "c19b1bf8-616d-4a15-87aa-4b90e4403904"
      },
      "source": [
        "apart_1 = extract_features('/content/drive/MyDrive/ds/pizza_images/autoencoder/test/veranda_test.jpg', model_finetuned1)\r\n",
        "apart_2 = extract_features('/content/drive/MyDrive/ds/pizza_images/autoencoder/test/veranda_test.jpg', model_finetuned2)\r\n",
        "apart_3 = extract_features('/content/drive/MyDrive/ds/pizza_images/autoencoder/test/veranda_test.jpg', model_finetuned3)\r\n",
        "print('Length of first feature set is:',len(apart_1))\r\n",
        "print('Apart would have the vector:',apart_1)\r\n",
        "print('Length of second feature set is:',len(apart_2))\r\n",
        "print('Apart would have the vector:',apart_2)\r\n",
        "print('Length of third feature set is:',len(apart_3))\r\n",
        "print('Apart would have the vector:',apart_3)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f43fae7f5f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Length of first feature set is: 1\n",
            "Apart would have the vector: [1.]\n",
            "Length of second feature set is: 2\n",
            "Apart would have the vector: [0.68838465 0.7253459 ]\n",
            "Length of third feature set is: 2\n",
            "Apart would have the vector: [0.76041836 0.6494335 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm8DDqFtTWaL"
      },
      "source": [
        "def model_maker2():\r\n",
        "    base_model = ResNet50(include_top=False,\r\n",
        "                           input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\r\n",
        "    for layer in base_model.layers[:]:\r\n",
        "        layer.trainable = False\r\n",
        "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\r\n",
        "    custom_model = base_model(input)\r\n",
        "    custom_model = GlobalAveragePooling2D()(custom_model)\r\n",
        "    custom_model= Dense(64, activation='relu', name='encoded_layer')(custom_model)\r\n",
        "    custom_model = Dropout(0.5)(custom_model)\r\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\r\n",
        "    return Model(inputs=input, outputs=predictions)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEH8qon2Tfjh",
        "outputId": "df7a9053-fa12-4b46-d225-c6892612c4b6"
      },
      "source": [
        "model_finetuned = model_maker2()\r\n",
        "model_finetuned.compile(loss='mse',\r\n",
        "              optimizer=tensorflow.keras.optimizers.Adam(0.001),\r\n",
        "              metrics=['acc'])\r\n",
        "model_finetuned.fit_generator(\r\n",
        "    train_generator,\r\n",
        "    steps_per_epoch=math.ceil(float(TRAIN_SAMPLES) / batch_size),\r\n",
        "    epochs=10)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 45s 6s/step - loss: 0.3700 - acc: 0.4993\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 37s 6s/step - loss: 0.2266 - acc: 0.7116\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 40s 6s/step - loss: 0.2056 - acc: 0.7218\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 38s 5s/step - loss: 0.1820 - acc: 0.7384\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 40s 6s/step - loss: 0.2147 - acc: 0.6981\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 41s 5s/step - loss: 0.1632 - acc: 0.7571\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 41s 6s/step - loss: 0.1672 - acc: 0.7803\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 37s 5s/step - loss: 0.1336 - acc: 0.8159\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 41s 6s/step - loss: 0.1562 - acc: 0.8011\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 36s 5s/step - loss: 0.1579 - acc: 0.8151\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f43f8c0d1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqkDl4ndAkuk"
      },
      "source": [
        "model_finetuned_encoded.save('/content/drive/My Drive/ds/pizza_images/autoencoder/finetuned_resnet_thickthin_encoded.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2C4QENUAU_87"
      },
      "source": [
        "feature_extractor2 = Model(inputs=model_finetuned2.input, outputs=model_finetuned2.get_layer('encoded_layer').output)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtTvfBS-YSqN"
      },
      "source": [
        "# Functions to convert images to 3D (image_dim, image_dim, 3) \r\n",
        "# or 4D (1, image_dim, image_dim, 3) tensors\r\n",
        "\r\n",
        "image_dimension = 224\r\n",
        "\r\n",
        "def image_to_3d_tensor(image_path):\r\n",
        "    im = cv2.resize(cv2.imread(image_path), (image_dimension, image_dimension)).astype(np.float32)\r\n",
        "    return im/255\r\n",
        "\r\n",
        "def image_to_4d_tensor(image_path):\r\n",
        "    im = image_to_3d_tensor(image_path)\r\n",
        "    im = im.reshape(1, image_dimension, image_dimension, 3)\r\n",
        "    return im"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9CFRpoHYmpc",
        "outputId": "607a772f-f0f3-4657-e19f-c82c8a2c263e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "apart_new = feature_extractor(image_to_4d_tensor('/content/drive/MyDrive/ds/pizza_images/autoencoder/test/veranda_test.jpg'), model_finetuned)\r\n",
        "print('Length of first feature set is:',len(apart_new[0]))\r\n",
        "print('Apart would have the vector:',apart_new)\r\n",
        "pickle.dump(apart_new,open('/content/drive/My Drive/ds/pizza_images/autoencoder/features_resnet_apart_new.pickle', 'wb'))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of first feature set is: 64\n",
            "Apart would have the vector: tf.Tensor(\n",
            "[[1.1191238  0.         0.4273147  0.         0.         1.0270787\n",
            "  0.3987035  0.         0.2897739  0.         0.         0.\n",
            "  0.         0.         0.         0.         1.5317378  0.09443825\n",
            "  0.         0.00446777 0.         0.         0.30784902 0.\n",
            "  0.         0.         0.         0.         1.8419851  0.\n",
            "  0.447735   0.         0.41226494 0.         0.         0.\n",
            "  0.         0.         0.         0.03716054 0.         0.\n",
            "  0.22442544 0.         0.         0.         0.         0.\n",
            "  1.7905525  0.         0.         0.         0.5338802  0.6811141\n",
            "  0.         0.         0.         0.8679581  0.         0.\n",
            "  1.1879715  0.         0.         0.        ]], shape=(1, 64), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NovgWyfBXnCl",
        "outputId": "9a492830-532c-418a-e875-b539c7840b3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "bc9152a379504a63a1c1e56b7204f79c",
            "e85a36896d1d4d53b7ff79ea1959b855",
            "1541b704570e48c48beb8502d9c4d6e4",
            "fcfaeaed3fd642608b8aa5dff2253913",
            "591483d42c5945a6b59fede3e5528284",
            "533d5777afc24e6996f467a9025501be",
            "eb3f38e391ea4da2a6f482041ab11f44",
            "2c0c786da6ad49958a513735e228e1f8"
          ]
        }
      },
      "source": [
        "# path to the your datasets\r\n",
        "yelp_dir = '/content/drive/MyDrive/ds/pizza_images/yelp_only/data/'\r\n",
        "yelp_filenames = sorted(get_file_list(yelp_dir))\r\n",
        "\r\n",
        "extracted_feature_list_yelp = []\r\n",
        "for i in tqdm_notebook(range(len(yelp_filenames))):\r\n",
        "    extracted_feature_list_yelp.append(feature_extractor2(image_to_4d_tensor(yelp_filenames[i]), model_finetuned2))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc9152a379504a63a1c1e56b7204f79c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1698.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCl1ZHOVExBF"
      },
      "source": [
        "pickle.dump(extracted_feature_list_yelp,open('/content/drive/My Drive/ds/pizza_images/autoencoder/moreepochs_features_tensors-' + model_architecture + '.pickle', 'wb'))"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7M04lecmwJA",
        "outputId": "a7969c46-cf4d-49ec-939e-74cfd6830f5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "for i, features in enumerate(extracted_feature_list_yelp):\r\n",
        "    extracted_feature_list_yelp[i] = features / norm(features)\r\n",
        "\r\n",
        "extracted_feature_list_yelp = extracted_feature_list_yelp.reshape(num_images, -1)\r\n",
        "# print(\"Num images   = \", len(new_generator.classes))\r\n",
        "print(\"Shape of feature_list = \", extracted_feature_list_yelp.shape)\r\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-e19b502c661a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mextracted_feature_list_yelp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mextracted_feature_list_yelp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextracted_feature_list_yelp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(\"Num images   = \", len(new_generator.classes))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of feature_list = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextracted_feature_list_yelp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erHTNBNMXnFh",
        "outputId": "416d1bdc-20c7-4da8-c8eb-35b1e385fc0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batch_size = 32\r\n",
        "new_datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_input)\r\n",
        "\r\n",
        "new_generator = new_datagen.flow_from_directory('//content/drive/MyDrive/ds/pizza_images/yelp_only/',\r\n",
        "                                        target_size=(224, 224),\r\n",
        "                                        batch_size=batch_size,\r\n",
        "                                        class_mode=None,\r\n",
        "                                        shuffle=False)\r\n",
        "\r\n",
        "num_images = len(new_generator.filenames)\r\n",
        "num_epochs = int(math.ceil(num_images / batch_size))\r\n",
        "\r\n",
        "start_time = time.time()\r\n",
        "feature_list_yelp = []\r\n",
        "# feature_list_yelp = feature_extractor(image_to_4d_tensor('/content/drive/MyDrive/ds/pizza_images/autoencoder/test/veranda_test.jpg'), model_finetuned)\r\n",
        "feature_list_yelp = model_finetuned.predict_generator(new_generator, num_epochs)\r\n",
        "end_time = time.time()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1698 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f43f6a65dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1PGkg2VXnIh",
        "outputId": "0bd67eb0-4687-47d7-8f8b-89b2331b8825",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for i, features in enumerate(feature_list_yelp):\r\n",
        "    feature_list_yelp[i] = features / norm(features)\r\n",
        "\r\n",
        "feature_list_yelp = feature_list_yelp.reshape(num_images, -1)\r\n",
        "print(\"Num images   = \", len(new_generator.classes))\r\n",
        "print(\"Shape of feature_list = \", feature_list_yelp.shape)\r\n",
        "print(\"Time taken in sec = \", end_time - start_time)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num images   =  1698\n",
            "Shape of feature_list =  (1698, 2)\n",
            "Time taken in sec =  258.47179317474365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8ABUUKVX7K0",
        "outputId": "c4ded8c8-7acd-4ec3-ffc3-f3a7d181f60c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "yelp_filenames = ['/content/drive/MyDrive/ds/pizza_images/yelp_only/' + s for s in new_generator.filenames]\r\n",
        "yelp_filenames[0]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/ds/pizza_images/yelp_only/data/als-pizza-chicago_0.jpeg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hjkjZcGlOQz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz5f8cJIX7Ng"
      },
      "source": [
        "pickle.dump(new_generator.classes, open('/content/drive/My Drive/ds/pizza_images/autoencoder/new_resnet_classids.pickle','wb'))\r\n",
        "pickle.dump(yelp_filenames, open('/content/drive/My Drive/ds/pizza_images/autoencoder/new_filenames_images.pickle', 'wb'))\r\n",
        "pickle.dump(feature_list_yelp,open('/content/drive/My Drive/ds/pizza_images/autoencoder/new_features-' + model_architecture + '.pickle', 'wb'))\r\n",
        "pickle.dump(extracted_feature_list_yelp,open('/content/drive/My Drive/ds/pizza_images/autoencoder/new_features_tensors-' + model_architecture + '.pickle', 'wb'))"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBuqPzHEVGwd"
      },
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cos3q1BomNHl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKykjLxNmNJ6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXm8kcWQmNVW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKG6WUZbVGzj"
      },
      "source": [
        "# Corpus directory\r\n",
        "corpus_image_directory = allimage_directory\r\n",
        "corpus_image_count = len([name for name in os.listdir(corpus_image_directory)])\r\n",
        "print(corpus_image_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vd-EXp5DVHDe"
      },
      "source": [
        "np.random.normal()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUgT9EymarV_",
        "outputId": "950d6390-143a-406d-9bfc-317706100ee6"
      },
      "source": [
        "feature_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\r\n",
        "feature_generator = feature_datagen.flow_from_directory('/content/drive/MyDrive/ds/pizza_images/yelp_only/',\r\n",
        "                                                    target_size=(IMG_WIDTH,\r\n",
        "                                                                 IMG_HEIGHT),\r\n",
        "                                                    batch_size=batch_size,\r\n",
        "                                                    shuffle=True,\r\n",
        "                                                    seed=12345,\r\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1698 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcEaWdgFU-Np"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uthO5agAVgR2",
        "outputId": "0869cca8-2c0c-453c-92eb-6882deb3bf1e"
      },
      "source": [
        "start_time = time.time()\r\n",
        "feature_list_finetuned = []\r\n",
        "\r\n",
        "num_images = len(feature_generator.filenames)\r\n",
        "num_epochs = int(math.ceil(num_images / batch_size))\r\n",
        "\r\n",
        "feature_list_finetuned = feature_extractor.predict_generator(feature_generator, num_epochs)\r\n",
        "end_time = time.time()\r\n",
        "\r\n",
        "for i, features_finetuned in enumerate(feature_list_finetuned):\r\n",
        "    feature_list_finetuned[i] = features_finetuned / norm(features_finetuned)\r\n",
        "\r\n",
        "feature_list_two = feature_list_finetuned.reshape(num_images, -1)\r\n",
        "\r\n",
        "print(\"Num images   = \", len(feature_generator.classes))\r\n",
        "print(\"Shape of feature_list = \", feature_list_two.shape)\r\n",
        "print(\"Time taken in sec = \", end_time - start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num images   =  1698\n",
            "Shape of feature_list =  (1698, 2)\n",
            "Time taken in sec =  309.4018552303314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12hzgpHAVkXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf25ab1-5db3-4630-be68-4da254b3bc3f"
      },
      "source": [
        "pickle.dump(feature_list_two, open('/content/drive/My Drive/ds/pizza_images/autoencoder/features-resnet-finetuned2.pickle', 'wb'))\r\n",
        "feature_list_two[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9302449 , 0.36693925], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG9j9hISXzfK"
      },
      "source": [
        "feature_list = pickle.load(open('/content/drive/My Drive/ds/pizza_images/autoencoder/features-resnet-finetuned2.pickle','rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q60SDZ-Y9Wew",
        "outputId": "ef7d00bc-b896-4e5a-f017-7e1a4e593aec"
      },
      "source": [
        "model_finetuned.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet50 (Functional)        (None, 7, 7, 2048)        23587712  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                131136    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 130       \n",
            "=================================================================\n",
            "Total params: 23,718,978\n",
            "Trainable params: 131,266\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIjqHDap1LXt"
      },
      "source": [
        "# Save feature extractor\r\n",
        "feature_extractor = Model(inputs=model_finetuned.input, outputs=model_finetuned.get_layer('dense').output)\r\n",
        "# feature_extractor\r\n",
        "# apart = feature_extractor('/content/drive/MyDrive/ds/pizza_images/autoencoder/test/veranda_test.jpg')\r\n",
        "# print(apart)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUCG5hgi1NCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8066e947-7520-4cb9-a6c8-9a2be4753d56"
      },
      "source": [
        "feature_extractor.save('/content/drive/My Drive/ds/pizza_images/autoencoder/feature_extractor_finetuned2.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1698"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ow-KBV5EP8Jx"
      },
      "source": [
        "layer_name = 'dense'\r\n",
        "intermediate_layer_model = keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\r\n",
        "intermediate_output = intermediate_layer_model(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwCz9pjsMzy1",
        "outputId": "c0ea1404-cf26-4dbd-a7b8-c6adfc9a89a9"
      },
      "source": [
        "scaler = transforms.Scale((224, 224))\r\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\r\n",
        "                                 std=[0.229, 0.224, 0.225])\r\n",
        "to_tensor = transforms.ToTensor()\r\n",
        "\r\n",
        "def get_vector(image_name,model):\r\n",
        "    # 1. Load the image with Pillow library\r\n",
        "    img = Image.open(image_name)\r\n",
        "    # 2. Create a PyTorch Variable with the transformed image\r\n",
        "    t_img = Variable(normalize(to_tensor(scaler(img))).unsqueeze(0))\r\n",
        "    # 3. Create a vector of zeros that will hold our feature vector\r\n",
        "    #    The 'avgpool' layer has an output size of 512\r\n",
        "    my_embedding = torch.zeros(512)\r\n",
        "    # 4. Define a function that will copy the output of a layer\r\n",
        "    def copy_data(m, i, o):\r\n",
        "        my_embedding.copy_(o.data)\r\n",
        "    # 5. Attach that function to our selected layer\r\n",
        "    layer_name = 'dense'\r\n",
        "    intermediate_layer_model = tensorflow.keras.Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\r\n",
        "    # 6. Run the model on our transformed image\r\n",
        "    intermediate_output = intermediate_layer_model(t_img)\r\n",
        "    # 8. Return the feature vector\r\n",
        "    return my_embedding"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py:285: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "wj3dyEBtNxNV",
        "outputId": "466b7e61-090c-4af5-c623-ce457c52f9a7"
      },
      "source": [
        "get_vector('/content/drive/MyDrive/ds/pizza_images/autoencoder/test/veranda_test.jpg', model_finetuned)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-113-2eefb66956bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/ds/pizza_images/autoencoder/test/veranda_test.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_finetuned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-112-6cbeaa9f9930>\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(image_name, model)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mintermediate_layer_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# 6. Run the model on our transformed image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintermediate_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;31m# 8. Return the feature vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmy_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m       \u001b[0minput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0meager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    272\u001b[0m                              \u001b[0;34m' is incompatible with layer '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlayer_name\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                              \u001b[0;34m': expected shape='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                              ', found shape=' + display_shape(x.shape))\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36mdisplay_shape\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdisplay_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'torch.Size' object has no attribute 'as_list'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFwAlrwI0J17"
      },
      "source": [
        "# Functions to convert images to 3D (image_dim, image_dim, 3) \r\n",
        "# or 4D (1, image_dim, image_dim, 3) tensors\r\n",
        "\r\n",
        "image_dimension = 224\r\n",
        "\r\n",
        "def image_to_3d_tensor(image_path):\r\n",
        "    im = cv2.resize(cv2.imread(image_path), (image_dimension, image_dimension)).astype(np.float32)\r\n",
        "    return im/255\r\n",
        "\r\n",
        "def image_to_4d_tensor(image_path):\r\n",
        "    im = image_to_3d_tensor(image_path)\r\n",
        "    im = im.reshape(1, image_dimension, image_dimension, 3)\r\n",
        "    return im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in1LG8RC7Sh9",
        "outputId": "0f84683c-4f0f-4552-9209-4bcc3b3710e4"
      },
      "source": [
        "# Corpus directory\r\n",
        "corpus_image_directory = '/content/drive/MyDrive/ds/pizza_images/yelp_only/data/'\r\n",
        "corpus_image_count = len([name for name in os.listdir(corpus_image_directory)])\r\n",
        "print(corpus_image_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrwmRplL_UpX"
      },
      "source": [
        "# Instantiate tensors array and list of filenames\r\n",
        "corpus_tensors = np.zeros((1, image_dimension, image_dimension, 3))\r\n",
        "corpus_tensors_list = []\r\n",
        "corpus_filenames = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJCh-3dHFP1O"
      },
      "source": [
        "# image_to_3d_tensor(corpus_image_directory + os.listdir(corpus_image_directory)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBVcOLYl_W5z",
        "outputId": "0c98b6f3-384d-4df6-c4c5-82c05d7a79c3"
      },
      "source": [
        "# Create image tensors and filename list\r\n",
        "for filename in os.listdir(corpus_image_directory):\r\n",
        "    corpus_filenames.append(filename)\r\n",
        "    try: \r\n",
        "        corpus_tensors_list.append(image_to_3d_tensor(corpus_image_directory + filename))\r\n",
        "    except: \r\n",
        "        pass\r\n",
        "    if len(corpus_tensors_list) % 50 == 0: print('Processed:', len(corpus_tensors_list)) \r\n",
        "corpus_tensors = np.array(corpus_tensors_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed: 50\n",
            "Processed: 100\n",
            "Processed: 150\n",
            "Processed: 200\n",
            "Processed: 250\n",
            "Processed: 300\n",
            "Processed: 350\n",
            "Processed: 400\n",
            "Processed: 450\n",
            "Processed: 500\n",
            "Processed: 550\n",
            "Processed: 600\n",
            "Processed: 650\n",
            "Processed: 700\n",
            "Processed: 750\n",
            "Processed: 800\n",
            "Processed: 850\n",
            "Processed: 900\n",
            "Processed: 950\n",
            "Processed: 1000\n",
            "Processed: 1050\n",
            "Processed: 1100\n",
            "Processed: 1150\n",
            "Processed: 1200\n",
            "Processed: 1250\n",
            "Processed: 1300\n",
            "Processed: 1350\n",
            "Processed: 1400\n",
            "Processed: 1450\n",
            "Processed: 1500\n",
            "Processed: 1550\n",
            "Processed: 1600\n",
            "Processed: 1650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeknKRXPFAW5",
        "outputId": "4191a5e1-913f-486f-f99a-98eb23590dec"
      },
      "source": [
        "test_image_file = '/content/drive/MyDrive/ds/pizza_images/autoencoder/test/veranda_test.jpg'\r\n",
        "test_tensor = image_to_4d_tensor(test_image_file)\r\n",
        "test_features = feature_extractor.predict(test_tensor,verbose=1)\r\n",
        "len(test_features[0])\r\n",
        "# print(test_features.shape[0])\r\n",
        "# print(test_features.shape[1] * test_features.shape[2] * test_features.shape[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 199ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZfZ-CVTJpEt"
      },
      "source": [
        "corpus_image_directory = '/content/drive/MyDrive/ds/pizza_images/yelp_only/data/'\r\n",
        "corpus_image_count = len([name for name in os.listdir(corpus_image_directory)])\r\n",
        "print(corpus_image_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z45qsVD1JpIJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h36FPs8FJpKy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98w2RyHRBKlh"
      },
      "source": [
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9FHFTA-_rIN"
      },
      "source": [
        "def compare_test_image(test_image_file, metric='cosine'):\r\n",
        "    \r\n",
        "    # Display original image\r\n",
        "    print('Original image')\r\n",
        "    im = Image.open(test_image_file)\r\n",
        "    im.show()\r\n",
        "    # display(Image(test_image_file, width = image_dimension, height = image_dimension))\r\n",
        "    \r\n",
        "    # Predict feature values for test image\r\n",
        "    test_tensor = image_to_4d_tensor(test_image_file)\r\n",
        "    test_features = feature_extractor.predict(test_tensor)\r\n",
        "    \r\n",
        "    # Use a vectorized function (spatial.distance.cdist) to calculate the distance between\r\n",
        "    # the test image and each of the images in the corpus.\r\n",
        "    # Images need to be converted to '2d' for use in distance.cdist.\r\n",
        "    # First dimension is number of images (1 for test, corpus size for corpus).\r\n",
        "    # Second dimension is width x height x number of color channels.\r\n",
        "    test_features_2d = np.reshape(test_features, (test_features.shape[0], test_features.shape[1] * test_features.shape[2] * test_features.shape[3]))\r\n",
        "    corpus_features_2d = np.reshape(corpus_features, (corpus_features.shape[0], corpus_features.shape[1] * corpus_features.shape[2] * corpus_features.shape[3]))\r\n",
        "    \r\n",
        "    # Run comparison\r\n",
        "    distance_array = spatial.distance.cdist(test_features_2d, corpus_features_2d, metric=metric)\r\n",
        "\r\n",
        "    # If #1 best match is indistinguishable from test image, skip \r\n",
        "    shift = 0\r\n",
        "    if np.min(distance_array) < .001: shift = 1\r\n",
        "        \r\n",
        "    # Return top 5 id numbers using argsort\r\n",
        "    top5ids = [corpus_filenames[index] for index in distance_array.argsort()[0][0+shift:5+shift].tolist()]\r\n",
        "    top5list = []\r\n",
        "    \r\n",
        "    return top5ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "vhQcDMmjGRv7",
        "outputId": "32f3a73b-6fb8-4a61-b2ea-41fabb220e54"
      },
      "source": [
        "# Execute comparison for test image.\r\n",
        "top5 = compare_test_image('/content/drive/MyDrive/ds/pizza_images/autoencoder/test/veranda_test.jpg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original image\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-64fe3f69aa47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Execute comparison for test image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtop5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare_test_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/ds/pizza_images/autoencoder/test/veranda_test.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-76-f0179c0083a7>\u001b[0m in \u001b[0;36mcompare_test_image\u001b[0;34m(test_image_file, metric)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# First dimension is number of images (1 for test, corpus size for corpus).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Second dimension is width x height x number of color channels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtest_features_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mcorpus_features_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcorpus_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorpus_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorpus_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m61-Q25uGavz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}